{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "300e1f81-be4f-4875-8e25-d77365a6bdbf",
   "metadata": {},
   "source": [
    "# Week 6 Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65149f72-17fa-46ba-9d3a-b54feb5a07da",
   "metadata": {},
   "source": [
    "#### We are required to train multiple machine learning models and evaluate their performance using metrics such as accuracy, precision, recall, and F1-score. Implement hyperparameter tuning techniques like GridSearchCV and RandomizedSearchCV to optimize model parameters. Analyze the results to select the best-performing model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10666ef9-3d09-4c23-9134-85d2f0fc1ab7",
   "metadata": {},
   "source": [
    "### Importing necessary libraries and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "98dafc0a-6283-4438-bd9c-3424bb431300",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "import optuna\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3f928892-99da-4458-ba0c-f57e908121c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8f3d3561-30e7-4ef9-98ea-b3367ac5bf8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4b67f71a-7454-4114-a2c6-e2e7ed7ce252",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_breast_cancer()\n",
    "x = data.data\n",
    "y = data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8ca011be-6f80-4537-b045-c5bac29259dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.799e+01, 1.038e+01, 1.228e+02, ..., 2.654e-01, 4.601e-01,\n",
       "        1.189e-01],\n",
       "       [2.057e+01, 1.777e+01, 1.329e+02, ..., 1.860e-01, 2.750e-01,\n",
       "        8.902e-02],\n",
       "       [1.969e+01, 2.125e+01, 1.300e+02, ..., 2.430e-01, 3.613e-01,\n",
       "        8.758e-02],\n",
       "       ...,\n",
       "       [1.660e+01, 2.808e+01, 1.083e+02, ..., 1.418e-01, 2.218e-01,\n",
       "        7.820e-02],\n",
       "       [2.060e+01, 2.933e+01, 1.401e+02, ..., 2.650e-01, 4.087e-01,\n",
       "        1.240e-01],\n",
       "       [7.760e+00, 2.454e+01, 4.792e+01, ..., 0.000e+00, 2.871e-01,\n",
       "        7.039e-02]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d7d0c14e-fb22-4fee-9188-59dd420654ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n",
       "       1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,\n",
       "       1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n",
       "       0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,\n",
       "       1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "       1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,\n",
       "       1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,\n",
       "       0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,\n",
       "       1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "       1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,\n",
       "       1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3ee145-605d-4ab4-ab8b-7418f642a3f1",
   "metadata": {},
   "source": [
    "### Splitting the data into training and testing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7cc068bf-40fd-486d-acd5-e01466f7e8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0daddd32-3d09-4301-9cd9-c28303556834",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1aa09a3f-12a0-4037-83ba-1569d71aba91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0d48bb73-7a8a-4f62-adfd-c86f8eca4a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-10 18:40:01,612] A new study created in memory with name: no-name-295a7105-f27c-4903-a43f-df381ba73976\n",
      "[I 2025-07-10 18:40:01,822] Trial 0 finished with value: 0.03508771929824561 and parameters: {'learning_rate': 0.09206773677618102, 'max_depth': 6, 'n_estimators': 831, 'min_child_weight': 1}. Best is trial 0 with value: 0.03508771929824561.\n",
      "[I 2025-07-10 18:40:02,016] Trial 1 finished with value: 0.01754385964912286 and parameters: {'learning_rate': 0.04186089565031615, 'max_depth': 5, 'n_estimators': 694, 'min_child_weight': 4}. Best is trial 1 with value: 0.01754385964912286.\n",
      "[I 2025-07-10 18:40:02,307] Trial 2 finished with value: 0.03508771929824561 and parameters: {'learning_rate': 0.08296423203963219, 'max_depth': 7, 'n_estimators': 981, 'min_child_weight': 2}. Best is trial 1 with value: 0.01754385964912286.\n",
      "[I 2025-07-10 18:40:02,348] Trial 3 finished with value: 0.03508771929824561 and parameters: {'learning_rate': 0.08987116811675695, 'max_depth': 3, 'n_estimators': 103, 'min_child_weight': 4}. Best is trial 1 with value: 0.01754385964912286.\n",
      "[I 2025-07-10 18:40:02,539] Trial 4 finished with value: 0.03508771929824561 and parameters: {'learning_rate': 0.0031220205947614316, 'max_depth': 3, 'n_estimators': 399, 'min_child_weight': 1}. Best is trial 1 with value: 0.01754385964912286.\n",
      "[I 2025-07-10 18:40:02,817] Trial 5 finished with value: 0.04385964912280704 and parameters: {'learning_rate': 0.01111063529169745, 'max_depth': 4, 'n_estimators': 559, 'min_child_weight': 2}. Best is trial 1 with value: 0.01754385964912286.\n",
      "[I 2025-07-10 18:40:03,064] Trial 6 finished with value: 0.01754385964912286 and parameters: {'learning_rate': 0.06708409958033194, 'max_depth': 4, 'n_estimators': 971, 'min_child_weight': 4}. Best is trial 1 with value: 0.01754385964912286.\n",
      "[I 2025-07-10 18:40:03,343] Trial 7 finished with value: 0.04385964912280704 and parameters: {'learning_rate': 0.046571113372749606, 'max_depth': 7, 'n_estimators': 826, 'min_child_weight': 1}. Best is trial 1 with value: 0.01754385964912286.\n",
      "[I 2025-07-10 18:40:03,632] Trial 8 finished with value: 0.04385964912280704 and parameters: {'learning_rate': 0.01829481106866153, 'max_depth': 6, 'n_estimators': 410, 'min_child_weight': 1}. Best is trial 1 with value: 0.01754385964912286.\n",
      "[I 2025-07-10 18:40:03,777] Trial 9 finished with value: 0.04385964912280704 and parameters: {'learning_rate': 0.03410168399829541, 'max_depth': 3, 'n_estimators': 335, 'min_child_weight': 1}. Best is trial 1 with value: 0.01754385964912286.\n",
      "[I 2025-07-10 18:40:04,056] Trial 10 finished with value: 0.01754385964912286 and parameters: {'learning_rate': 0.061332910543513905, 'max_depth': 5, 'n_estimators': 645, 'min_child_weight': 5}. Best is trial 1 with value: 0.01754385964912286.\n",
      "[I 2025-07-10 18:40:04,359] Trial 11 finished with value: 0.01754385964912286 and parameters: {'learning_rate': 0.06713340235168028, 'max_depth': 5, 'n_estimators': 941, 'min_child_weight': 4}. Best is trial 1 with value: 0.01754385964912286.\n",
      "[I 2025-07-10 18:40:04,617] Trial 12 finished with value: 0.01754385964912286 and parameters: {'learning_rate': 0.0341900009537561, 'max_depth': 4, 'n_estimators': 720, 'min_child_weight': 4}. Best is trial 1 with value: 0.01754385964912286.\n",
      "[I 2025-07-10 18:40:04,816] Trial 13 finished with value: 0.01754385964912286 and parameters: {'learning_rate': 0.06955381519222324, 'max_depth': 4, 'n_estimators': 785, 'min_child_weight': 5}. Best is trial 1 with value: 0.01754385964912286.\n",
      "[I 2025-07-10 18:40:05,026] Trial 14 finished with value: 0.03508771929824561 and parameters: {'learning_rate': 0.04868785739338099, 'max_depth': 5, 'n_estimators': 546, 'min_child_weight': 3}. Best is trial 1 with value: 0.01754385964912286.\n",
      "[I 2025-07-10 18:40:05,392] Trial 15 finished with value: 0.03508771929824561 and parameters: {'learning_rate': 0.03419654927333904, 'max_depth': 6, 'n_estimators': 906, 'min_child_weight': 3}. Best is trial 1 with value: 0.01754385964912286.\n",
      "[I 2025-07-10 18:40:05,616] Trial 16 finished with value: 0.01754385964912286 and parameters: {'learning_rate': 0.07820919217203291, 'max_depth': 4, 'n_estimators': 644, 'min_child_weight': 4}. Best is trial 1 with value: 0.01754385964912286.\n",
      "[I 2025-07-10 18:40:05,971] Trial 17 finished with value: 0.01754385964912286 and parameters: {'learning_rate': 0.056829330682244134, 'max_depth': 4, 'n_estimators': 992, 'min_child_weight': 5}. Best is trial 1 with value: 0.01754385964912286.\n",
      "[I 2025-07-10 18:40:06,156] Trial 18 finished with value: 0.04385964912280704 and parameters: {'learning_rate': 0.04429236746386381, 'max_depth': 5, 'n_estimators': 221, 'min_child_weight': 3}. Best is trial 1 with value: 0.01754385964912286.\n",
      "[I 2025-07-10 18:40:06,440] Trial 19 finished with value: 0.02631578947368418 and parameters: {'learning_rate': 0.020853691555546637, 'max_depth': 5, 'n_estimators': 721, 'min_child_weight': 4}. Best is trial 1 with value: 0.01754385964912286.\n",
      "[I 2025-07-10 18:40:06,619] Trial 20 finished with value: 0.03508771929824561 and parameters: {'learning_rate': 0.09922832423680686, 'max_depth': 6, 'n_estimators': 536, 'min_child_weight': 2}. Best is trial 1 with value: 0.01754385964912286.\n",
      "[I 2025-07-10 18:40:06,791] Trial 21 finished with value: 0.01754385964912286 and parameters: {'learning_rate': 0.06089508414318088, 'max_depth': 5, 'n_estimators': 656, 'min_child_weight': 5}. Best is trial 1 with value: 0.01754385964912286.\n",
      "[I 2025-07-10 18:40:06,965] Trial 22 finished with value: 0.01754385964912286 and parameters: {'learning_rate': 0.05774209733730872, 'max_depth': 5, 'n_estimators': 618, 'min_child_weight': 5}. Best is trial 1 with value: 0.01754385964912286.\n",
      "[I 2025-07-10 18:40:07,115] Trial 23 finished with value: 0.01754385964912286 and parameters: {'learning_rate': 0.07189379990012335, 'max_depth': 4, 'n_estimators': 478, 'min_child_weight': 5}. Best is trial 1 with value: 0.01754385964912286.\n",
      "[I 2025-07-10 18:40:07,392] Trial 24 finished with value: 0.01754385964912286 and parameters: {'learning_rate': 0.040144299267259494, 'max_depth': 5, 'n_estimators': 886, 'min_child_weight': 4}. Best is trial 1 with value: 0.01754385964912286.\n",
      "[I 2025-07-10 18:40:07,669] Trial 25 finished with value: 0.01754385964912286 and parameters: {'learning_rate': 0.06222345210835142, 'max_depth': 6, 'n_estimators': 762, 'min_child_weight': 4}. Best is trial 1 with value: 0.01754385964912286.\n",
      "[I 2025-07-10 18:40:07,874] Trial 26 finished with value: 0.01754385964912286 and parameters: {'learning_rate': 0.05276711023341085, 'max_depth': 4, 'n_estimators': 704, 'min_child_weight': 5}. Best is trial 1 with value: 0.01754385964912286.\n",
      "[I 2025-07-10 18:40:08,103] Trial 27 finished with value: 0.03508771929824561 and parameters: {'learning_rate': 0.07740008928267839, 'max_depth': 5, 'n_estimators': 856, 'min_child_weight': 3}. Best is trial 1 with value: 0.01754385964912286.\n",
      "[I 2025-07-10 18:40:08,276] Trial 28 finished with value: 0.01754385964912286 and parameters: {'learning_rate': 0.06595410090106818, 'max_depth': 3, 'n_estimators': 588, 'min_child_weight': 4}. Best is trial 1 with value: 0.01754385964912286.\n",
      "[I 2025-07-10 18:40:08,437] Trial 29 finished with value: 0.03508771929824561 and parameters: {'learning_rate': 0.0285762909140516, 'max_depth': 6, 'n_estimators': 480, 'min_child_weight': 5}. Best is trial 1 with value: 0.01754385964912286.\n",
      "[I 2025-07-10 18:40:08,658] Trial 30 finished with value: 0.02631578947368418 and parameters: {'learning_rate': 0.05416411109582719, 'max_depth': 4, 'n_estimators': 785, 'min_child_weight': 3}. Best is trial 1 with value: 0.01754385964912286.\n",
      "[I 2025-07-10 18:40:08,900] Trial 31 finished with value: 0.01754385964912286 and parameters: {'learning_rate': 0.07572587421541473, 'max_depth': 5, 'n_estimators': 935, 'min_child_weight': 4}. Best is trial 1 with value: 0.01754385964912286.\n",
      "[I 2025-07-10 18:40:09,145] Trial 32 finished with value: 0.02631578947368418 and parameters: {'learning_rate': 0.08315305535018105, 'max_depth': 5, 'n_estimators': 952, 'min_child_weight': 4}. Best is trial 1 with value: 0.01754385964912286.\n",
      "[I 2025-07-10 18:40:09,444] Trial 33 finished with value: 0.01754385964912286 and parameters: {'learning_rate': 0.06593067832657207, 'max_depth': 7, 'n_estimators': 852, 'min_child_weight': 4}. Best is trial 1 with value: 0.01754385964912286.\n",
      "[I 2025-07-10 18:40:09,687] Trial 34 finished with value: 0.03508771929824561 and parameters: {'learning_rate': 0.08790697043427559, 'max_depth': 5, 'n_estimators': 957, 'min_child_weight': 3}. Best is trial 1 with value: 0.01754385964912286.\n",
      "[I 2025-07-10 18:40:09,921] Trial 35 finished with value: 0.01754385964912286 and parameters: {'learning_rate': 0.04286170225085417, 'max_depth': 5, 'n_estimators': 690, 'min_child_weight': 5}. Best is trial 1 with value: 0.01754385964912286.\n",
      "[I 2025-07-10 18:40:10,120] Trial 36 finished with value: 0.01754385964912286 and parameters: {'learning_rate': 0.06854292475796153, 'max_depth': 6, 'n_estimators': 808, 'min_child_weight': 4}. Best is trial 1 with value: 0.01754385964912286.\n",
      "[I 2025-07-10 18:40:10,359] Trial 37 finished with value: 0.03508771929824561 and parameters: {'learning_rate': 0.05115802416592195, 'max_depth': 3, 'n_estimators': 748, 'min_child_weight': 2}. Best is trial 1 with value: 0.01754385964912286.\n",
      "[I 2025-07-10 18:40:10,583] Trial 38 finished with value: 0.01754385964912286 and parameters: {'learning_rate': 0.06197389085776233, 'max_depth': 4, 'n_estimators': 887, 'min_child_weight': 4}. Best is trial 1 with value: 0.01754385964912286.\n",
      "[I 2025-07-10 18:40:10,715] Trial 39 finished with value: 0.01754385964912286 and parameters: {'learning_rate': 0.08500449913488241, 'max_depth': 7, 'n_estimators': 283, 'min_child_weight': 5}. Best is trial 1 with value: 0.01754385964912286.\n",
      "[I 2025-07-10 18:40:11,159] Trial 40 finished with value: 0.03508771929824561 and parameters: {'learning_rate': 0.07404489027277493, 'max_depth': 4, 'n_estimators': 998, 'min_child_weight': 3}. Best is trial 1 with value: 0.01754385964912286.\n",
      "[I 2025-07-10 18:40:11,425] Trial 41 finished with value: 0.01754385964912286 and parameters: {'learning_rate': 0.03771961068073123, 'max_depth': 4, 'n_estimators': 654, 'min_child_weight': 4}. Best is trial 1 with value: 0.01754385964912286.\n",
      "[I 2025-07-10 18:40:11,642] Trial 42 finished with value: 0.02631578947368418 and parameters: {'learning_rate': 0.03073973143500027, 'max_depth': 3, 'n_estimators': 596, 'min_child_weight': 4}. Best is trial 1 with value: 0.01754385964912286.\n",
      "[I 2025-07-10 18:40:11,906] Trial 43 finished with value: 0.02631578947368418 and parameters: {'learning_rate': 0.024723610801288515, 'max_depth': 4, 'n_estimators': 820, 'min_child_weight': 4}. Best is trial 1 with value: 0.01754385964912286.\n",
      "[I 2025-07-10 18:40:12,160] Trial 44 finished with value: 0.04385964912280704 and parameters: {'learning_rate': 0.010782323206187723, 'max_depth': 5, 'n_estimators': 505, 'min_child_weight': 3}. Best is trial 1 with value: 0.01754385964912286.\n",
      "[I 2025-07-10 18:40:12,392] Trial 45 finished with value: 0.01754385964912286 and parameters: {'learning_rate': 0.048711760547786725, 'max_depth': 4, 'n_estimators': 918, 'min_child_weight': 5}. Best is trial 1 with value: 0.01754385964912286.\n",
      "[I 2025-07-10 18:40:12,490] Trial 46 finished with value: 0.04385964912280704 and parameters: {'learning_rate': 0.039105064721520776, 'max_depth': 5, 'n_estimators': 117, 'min_child_weight': 4}. Best is trial 1 with value: 0.01754385964912286.\n",
      "[I 2025-07-10 18:40:12,737] Trial 47 finished with value: 0.03508771929824561 and parameters: {'learning_rate': 0.05686987361927429, 'max_depth': 4, 'n_estimators': 724, 'min_child_weight': 3}. Best is trial 1 with value: 0.01754385964912286.\n",
      "[I 2025-07-10 18:40:13,022] Trial 48 finished with value: 0.01754385964912286 and parameters: {'learning_rate': 0.04535949202002379, 'max_depth': 6, 'n_estimators': 673, 'min_child_weight': 5}. Best is trial 1 with value: 0.01754385964912286.\n",
      "[I 2025-07-10 18:40:13,166] Trial 49 finished with value: 0.01754385964912286 and parameters: {'learning_rate': 0.09380280503617092, 'max_depth': 3, 'n_estimators': 427, 'min_child_weight': 4}. Best is trial 1 with value: 0.01754385964912286.\n",
      "[I 2025-07-10 18:40:13,414] Trial 50 finished with value: 0.01754385964912286 and parameters: {'learning_rate': 0.034554215275314416, 'max_depth': 5, 'n_estimators': 764, 'min_child_weight': 4}. Best is trial 1 with value: 0.01754385964912286.\n",
      "[I 2025-07-10 18:40:13,587] Trial 51 finished with value: 0.01754385964912286 and parameters: {'learning_rate': 0.06450094824805314, 'max_depth': 4, 'n_estimators': 624, 'min_child_weight': 5}. Best is trial 1 with value: 0.01754385964912286.\n",
      "[I 2025-07-10 18:40:13,812] Trial 52 finished with value: 0.01754385964912286 and parameters: {'learning_rate': 0.07261717387523926, 'max_depth': 4, 'n_estimators': 858, 'min_child_weight': 5}. Best is trial 1 with value: 0.01754385964912286.\n",
      "[I 2025-07-10 18:40:13,998] Trial 53 finished with value: 0.01754385964912286 and parameters: {'learning_rate': 0.07997457371052599, 'max_depth': 4, 'n_estimators': 793, 'min_child_weight': 5}. Best is trial 1 with value: 0.01754385964912286.\n",
      "[I 2025-07-10 18:40:14,182] Trial 54 finished with value: 0.01754385964912286 and parameters: {'learning_rate': 0.06902992363173698, 'max_depth': 5, 'n_estimators': 740, 'min_child_weight': 5}. Best is trial 1 with value: 0.01754385964912286.\n",
      "[I 2025-07-10 18:40:14,446] Trial 55 finished with value: 0.01754385964912286 and parameters: {'learning_rate': 0.0591562550104487, 'max_depth': 5, 'n_estimators': 970, 'min_child_weight': 4}. Best is trial 1 with value: 0.01754385964912286.\n",
      "[I 2025-07-10 18:40:14,748] Trial 56 finished with value: 0.01754385964912286 and parameters: {'learning_rate': 0.06737052894251573, 'max_depth': 4, 'n_estimators': 889, 'min_child_weight': 5}. Best is trial 1 with value: 0.01754385964912286.\n",
      "[I 2025-07-10 18:40:14,956] Trial 57 finished with value: 0.01754385964912286 and parameters: {'learning_rate': 0.05352931015576823, 'max_depth': 3, 'n_estimators': 584, 'min_child_weight': 4}. Best is trial 1 with value: 0.01754385964912286.\n",
      "[I 2025-07-10 18:40:15,170] Trial 58 finished with value: 0.02631578947368418 and parameters: {'learning_rate': 0.07184118564142723, 'max_depth': 4, 'n_estimators': 690, 'min_child_weight': 3}. Best is trial 1 with value: 0.01754385964912286.\n",
      "[I 2025-07-10 18:40:15,394] Trial 59 finished with value: 0.01754385964912286 and parameters: {'learning_rate': 0.07935788451014143, 'max_depth': 5, 'n_estimators': 833, 'min_child_weight': 5}. Best is trial 1 with value: 0.01754385964912286.\n",
      "[I 2025-07-10 18:40:15,620] Trial 60 finished with value: 0.03508771929824561 and parameters: {'learning_rate': 0.015551889709883259, 'max_depth': 6, 'n_estimators': 545, 'min_child_weight': 4}. Best is trial 1 with value: 0.01754385964912286.\n",
      "[I 2025-07-10 18:40:15,804] Trial 61 finished with value: 0.01754385964912286 and parameters: {'learning_rate': 0.06267337638869715, 'max_depth': 4, 'n_estimators': 643, 'min_child_weight': 4}. Best is trial 1 with value: 0.01754385964912286.\n",
      "[I 2025-07-10 18:40:15,994] Trial 62 finished with value: 0.01754385964912286 and parameters: {'learning_rate': 0.07010864532865113, 'max_depth': 4, 'n_estimators': 714, 'min_child_weight': 4}. Best is trial 1 with value: 0.01754385964912286.\n",
      "[I 2025-07-10 18:40:16,318] Trial 63 finished with value: 0.02631578947368418 and parameters: {'learning_rate': 0.07936702742026586, 'max_depth': 5, 'n_estimators': 624, 'min_child_weight': 4}. Best is trial 1 with value: 0.01754385964912286.\n",
      "[I 2025-07-10 18:40:16,624] Trial 64 finished with value: 0.04385964912280704 and parameters: {'learning_rate': 0.07565915119586203, 'max_depth': 4, 'n_estimators': 771, 'min_child_weight': 1}. Best is trial 1 with value: 0.01754385964912286.\n",
      "[I 2025-07-10 18:40:16,811] Trial 65 finished with value: 0.01754385964912286 and parameters: {'learning_rate': 0.0591641255484342, 'max_depth': 3, 'n_estimators': 676, 'min_child_weight': 5}. Best is trial 1 with value: 0.01754385964912286.\n",
      "[I 2025-07-10 18:40:17,080] Trial 66 finished with value: 0.03508771929824561 and parameters: {'learning_rate': 0.05564071411457995, 'max_depth': 5, 'n_estimators': 927, 'min_child_weight': 2}. Best is trial 1 with value: 0.01754385964912286.\n",
      "[I 2025-07-10 18:40:17,264] Trial 67 finished with value: 0.01754385964912286 and parameters: {'learning_rate': 0.04304484546498364, 'max_depth': 4, 'n_estimators': 576, 'min_child_weight': 4}. Best is trial 1 with value: 0.01754385964912286.\n",
      "[I 2025-07-10 18:40:17,538] Trial 68 finished with value: 0.07017543859649122 and parameters: {'learning_rate': 0.0006359092674782518, 'max_depth': 5, 'n_estimators': 514, 'min_child_weight': 5}. Best is trial 1 with value: 0.01754385964912286.\n",
      "[I 2025-07-10 18:40:17,745] Trial 69 finished with value: 0.01754385964912286 and parameters: {'learning_rate': 0.04974310407906264, 'max_depth': 4, 'n_estimators': 710, 'min_child_weight': 4}. Best is trial 1 with value: 0.01754385964912286.\n",
      "[I 2025-07-10 18:40:17,936] Trial 70 finished with value: 0.03508771929824561 and parameters: {'learning_rate': 0.08633255269671788, 'max_depth': 6, 'n_estimators': 643, 'min_child_weight': 3}. Best is trial 1 with value: 0.01754385964912286.\n",
      "[I 2025-07-10 18:40:18,216] Trial 71 finished with value: 0.01754385964912286 and parameters: {'learning_rate': 0.06519011314458582, 'max_depth': 4, 'n_estimators': 998, 'min_child_weight': 5}. Best is trial 1 with value: 0.01754385964912286.\n",
      "[I 2025-07-10 18:40:18,520] Trial 72 finished with value: 0.01754385964912286 and parameters: {'learning_rate': 0.029172524010893007, 'max_depth': 4, 'n_estimators': 947, 'min_child_weight': 5}. Best is trial 1 with value: 0.01754385964912286.\n",
      "[I 2025-07-10 18:40:18,758] Trial 73 finished with value: 0.01754385964912286 and parameters: {'learning_rate': 0.06075116363665025, 'max_depth': 4, 'n_estimators': 902, 'min_child_weight': 5}. Best is trial 1 with value: 0.01754385964912286.\n",
      "[I 2025-07-10 18:40:19,037] Trial 74 finished with value: 0.01754385964912286 and parameters: {'learning_rate': 0.04695053235392309, 'max_depth': 5, 'n_estimators': 976, 'min_child_weight': 4}. Best is trial 1 with value: 0.01754385964912286.\n",
      "[I 2025-07-10 18:40:19,297] Trial 75 finished with value: 0.01754385964912286 and parameters: {'learning_rate': 0.03593342589771192, 'max_depth': 3, 'n_estimators': 861, 'min_child_weight': 5}. Best is trial 1 with value: 0.01754385964912286.\n",
      "[I 2025-07-10 18:40:19,525] Trial 76 finished with value: 0.01754385964912286 and parameters: {'learning_rate': 0.0415395200989119, 'max_depth': 4, 'n_estimators': 607, 'min_child_weight': 4}. Best is trial 1 with value: 0.01754385964912286.\n",
      "[I 2025-07-10 18:40:19,802] Trial 77 finished with value: 0.01754385964912286 and parameters: {'learning_rate': 0.031674399335924135, 'max_depth': 5, 'n_estimators': 746, 'min_child_weight': 5}. Best is trial 1 with value: 0.01754385964912286.\n",
      "[I 2025-07-10 18:40:20,146] Trial 78 finished with value: 0.02631578947368418 and parameters: {'learning_rate': 0.0520287812634839, 'max_depth': 4, 'n_estimators': 805, 'min_child_weight': 3}. Best is trial 1 with value: 0.01754385964912286.\n",
      "[I 2025-07-10 18:40:20,484] Trial 79 finished with value: 0.01754385964912286 and parameters: {'learning_rate': 0.024965525251016307, 'max_depth': 5, 'n_estimators': 939, 'min_child_weight': 4}. Best is trial 1 with value: 0.01754385964912286.\n",
      "[I 2025-07-10 18:40:20,687] Trial 80 finished with value: 0.01754385964912286 and parameters: {'learning_rate': 0.06437008145763803, 'max_depth': 4, 'n_estimators': 564, 'min_child_weight': 4}. Best is trial 1 with value: 0.01754385964912286.\n",
      "[I 2025-07-10 18:40:20,890] Trial 81 finished with value: 0.01754385964912286 and parameters: {'learning_rate': 0.056535413704875616, 'max_depth': 5, 'n_estimators': 658, 'min_child_weight': 5}. Best is trial 1 with value: 0.01754385964912286.\n",
      "[I 2025-07-10 18:40:21,087] Trial 82 finished with value: 0.01754385964912286 and parameters: {'learning_rate': 0.07090623245187777, 'max_depth': 5, 'n_estimators': 688, 'min_child_weight': 5}. Best is trial 1 with value: 0.01754385964912286.\n",
      "[I 2025-07-10 18:40:21,308] Trial 83 finished with value: 0.01754385964912286 and parameters: {'learning_rate': 0.06793078412980487, 'max_depth': 5, 'n_estimators': 732, 'min_child_weight': 5}. Best is trial 1 with value: 0.01754385964912286.\n",
      "[I 2025-07-10 18:40:21,567] Trial 84 finished with value: 0.01754385964912286 and parameters: {'learning_rate': 0.060480746581777045, 'max_depth': 4, 'n_estimators': 972, 'min_child_weight': 5}. Best is trial 1 with value: 0.01754385964912286.\n",
      "[I 2025-07-10 18:40:21,842] Trial 85 finished with value: 0.01754385964912286 and parameters: {'learning_rate': 0.07569634641955407, 'max_depth': 5, 'n_estimators': 659, 'min_child_weight': 4}. Best is trial 1 with value: 0.01754385964912286.\n",
      "[I 2025-07-10 18:40:22,083] Trial 86 finished with value: 0.01754385964912286 and parameters: {'learning_rate': 0.0820517833661585, 'max_depth': 5, 'n_estimators': 1000, 'min_child_weight': 5}. Best is trial 1 with value: 0.01754385964912286.\n",
      "[I 2025-07-10 18:40:22,315] Trial 87 finished with value: 0.01754385964912286 and parameters: {'learning_rate': 0.0911157137414219, 'max_depth': 6, 'n_estimators': 785, 'min_child_weight': 5}. Best is trial 1 with value: 0.01754385964912286.\n",
      "[I 2025-07-10 18:40:22,542] Trial 88 finished with value: 0.02631578947368418 and parameters: {'learning_rate': 0.06262500568253698, 'max_depth': 4, 'n_estimators': 877, 'min_child_weight': 4}. Best is trial 1 with value: 0.01754385964912286.\n",
      "[I 2025-07-10 18:40:22,707] Trial 89 finished with value: 0.01754385964912286 and parameters: {'learning_rate': 0.07333393616371442, 'max_depth': 4, 'n_estimators': 618, 'min_child_weight': 5}. Best is trial 1 with value: 0.01754385964912286.\n",
      "[I 2025-07-10 18:40:22,953] Trial 90 finished with value: 0.01754385964912286 and parameters: {'learning_rate': 0.04721768374044274, 'max_depth': 7, 'n_estimators': 916, 'min_child_weight': 4}. Best is trial 1 with value: 0.01754385964912286.\n",
      "[I 2025-07-10 18:40:23,123] Trial 91 finished with value: 0.01754385964912286 and parameters: {'learning_rate': 0.0663575538707125, 'max_depth': 5, 'n_estimators': 626, 'min_child_weight': 5}. Best is trial 1 with value: 0.01754385964912286.\n",
      "[I 2025-07-10 18:40:23,327] Trial 92 finished with value: 0.01754385964912286 and parameters: {'learning_rate': 0.0535449899773651, 'max_depth': 5, 'n_estimators': 695, 'min_child_weight': 5}. Best is trial 1 with value: 0.01754385964912286.\n",
      "[I 2025-07-10 18:40:23,570] Trial 93 finished with value: 0.01754385964912286 and parameters: {'learning_rate': 0.05840710162569676, 'max_depth': 5, 'n_estimators': 602, 'min_child_weight': 5}. Best is trial 1 with value: 0.01754385964912286.\n",
      "[I 2025-07-10 18:40:23,761] Trial 94 finished with value: 0.01754385964912286 and parameters: {'learning_rate': 0.056441294871418586, 'max_depth': 5, 'n_estimators': 670, 'min_child_weight': 5}. Best is trial 1 with value: 0.01754385964912286.\n",
      "[I 2025-07-10 18:40:23,944] Trial 95 finished with value: 0.01754385964912286 and parameters: {'learning_rate': 0.0632308010651415, 'max_depth': 4, 'n_estimators': 648, 'min_child_weight': 5}. Best is trial 1 with value: 0.01754385964912286.\n",
      "[I 2025-07-10 18:40:24,165] Trial 96 finished with value: 0.01754385964912286 and parameters: {'learning_rate': 0.03775784663861941, 'max_depth': 5, 'n_estimators': 719, 'min_child_weight': 4}. Best is trial 1 with value: 0.01754385964912286.\n",
      "[I 2025-07-10 18:40:24,394] Trial 97 finished with value: 0.01754385964912286 and parameters: {'learning_rate': 0.05097495033341627, 'max_depth': 4, 'n_estimators': 827, 'min_child_weight': 4}. Best is trial 1 with value: 0.01754385964912286.\n",
      "[I 2025-07-10 18:40:24,610] Trial 98 finished with value: 0.01754385964912286 and parameters: {'learning_rate': 0.06908500797876932, 'max_depth': 5, 'n_estimators': 957, 'min_child_weight': 5}. Best is trial 1 with value: 0.01754385964912286.\n",
      "[I 2025-07-10 18:40:24,854] Trial 99 finished with value: 0.02631578947368418 and parameters: {'learning_rate': 0.058487588060156, 'max_depth': 5, 'n_estimators': 755, 'min_child_weight': 3}. Best is trial 1 with value: 0.01754385964912286.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters:  {'learning_rate': 0.04186089565031615, 'max_depth': 5, 'n_estimators': 694, 'min_child_weight': 4}\n",
      "Best score:  0.9824561403508771\n",
      "Test set accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "def objective(trial):\n",
    "    # Define the hyperparameters to tune\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-1)\n",
    "    max_depth = trial.suggest_int(\"max_depth\", 3, 7)\n",
    "    n_estimators = trial.suggest_int(\"n_estimators\", 100, 1000)\n",
    "    min_child_weight = trial.suggest_int(\"min_child_weight\", 1, 5)\n",
    "    \n",
    "    # Create an XGBoost classifier\n",
    "    clf = XGBClassifier(\n",
    "        learning_rate=learning_rate, \n",
    "        max_depth=max_depth,\n",
    "        n_estimators=n_estimators, \n",
    "        min_child_weight=min_child_weight\n",
    "    )\n",
    "    \n",
    "    # Train the classifier and calculate the accuracy on the validation set\n",
    "    clf.fit(x_train, y_train)\n",
    "    score = clf.score(x_test, y_test)\n",
    "    return 1.0 - score\n",
    "\n",
    "# Use Optuna to tune the hyperparameters\n",
    "study = optuna.create_study()\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "# Print the best hyperparameters and the best score\n",
    "print(\"Best hyperparameters: \", study.best_params)\n",
    "print(\"Best score: \", 1.0 - study.best_value)\n",
    "\n",
    "\n",
    "# Train the classifier with the best hyperparameters on the full training set\n",
    "best_params = study.best_params\n",
    "clf = XGBClassifier(\n",
    "    learning_rate=best_params[\"learning_rate\"], \n",
    "    max_depth=best_params[\"max_depth\"],\n",
    "    n_estimators=best_params[\"n_estimators\"], \n",
    "    min_child_weight=best_params[\"min_child_weight\"]\n",
    ")\n",
    "clf.fit(x, y)\n",
    "\n",
    "# Evaluate the tuned classifier on the test set\n",
    "score = clf.score(x_test, y_test)\n",
    "print(\"Test set accuracy: \", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4a084e-8cd3-4bcb-bc3a-df53436a3eac",
   "metadata": {},
   "source": [
    "### Tuning Random Forest, Logistic Regression and Support Vector Classifier(SVC) with optuna "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6fd81dd7-a966-4f57-91bd-685828a6312f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "371cd427-79b8-48a3-97c5-0a7010745ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining models\n",
    "models = [\n",
    "    LogisticRegression(),\n",
    "    SVC(),\n",
    "    RandomForestClassifier()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2090932c-6112-43ce-ab08-e320e7a8ec25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store model results\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2bdf15b6-43bf-4c45-8a91-a1e3bf363d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lavan\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    # Fit the model on the training data\n",
    "    model.fit(x_train, y_train)\n",
    "    \n",
    "    # Make predictions on the test set\n",
    "    y_pred = model.predict(x_test)\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    # Store results\n",
    "    results[model.__class__.__name__] = {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b50a0872-cf1f-4545-95fc-938ffb03deb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_test = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "414e4b17-aba6-4fbf-ae62-f650f5cb900e",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (1976480498.py, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[74], line 8\u001b[1;36m\u001b[0m\n\u001b[1;33m    'solver': ['liblinear', 'lbfgs']\u001b[0m\n\u001b[1;37m              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "# Logistic Regression\n",
    "for model in models:\n",
    "    if isinstance(model, LogisticRegression):\n",
    "        param_grid = {\n",
    "            'C': [0.01, 0.1, 1, 10],\n",
    "            'solver': ['liblinear', 'lbfgs']\n",
    "            'max_iter': [100, 200, 500]\n",
    "        }\n",
    "        model = GridSearchCV(model, param_grid, cv=3, scoring='accuracy') # GridSearch CV\n",
    "# SVC\n",
    "    elif isinstance(model, SVC):\n",
    "        param_dist = {\n",
    "            'C': [0.1, 1, 10],\n",
    "            'kernel': ['linear', 'rbf'],\n",
    "            'gamma': ['scale', 'auto']\n",
    "        }\n",
    "        model = RandomizedSearchCV(model, param_dist, n_iter=5, cv=3, scoring='accuracy') # RandomizedSearch CV\n",
    "# Random Forest Classifier\n",
    "    elif isinstance(model, RandomForestClassifier):\n",
    "        param_grid = {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'max_depth': [None, 10, 20],\n",
    "            'min_samples_split': [2, 5],\n",
    "        }\n",
    "        model = GridSearchCV(model, param_grid, cv=3, scoring='accuracy')\n",
    "\n",
    "    # Fit the model on the training data\n",
    "    model.fit(x_train, y_train)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred = model.predict(x_test)\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    # Store results\n",
    "    results[model.best_estimator_.__class__.__name__] = {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f89b4763-3709-4850-bfc4-9f6d382dc74d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression:\n",
      "accuracy: 0.956140350877193\n",
      "precision: 0.9459459459459459\n",
      "recall: 0.9859154929577465\n",
      "f1: 0.9655172413793104\n",
      "SVC:\n",
      "accuracy: 0.956140350877193\n",
      "precision: 0.9459459459459459\n",
      "recall: 0.9859154929577465\n",
      "f1: 0.9655172413793104\n",
      "RandomForestClassifier:\n",
      "accuracy: 0.9649122807017544\n",
      "precision: 0.958904109589041\n",
      "recall: 0.9859154929577465\n",
      "f1: 0.9722222222222222\n"
     ]
    }
   ],
   "source": [
    "for model_name, metrics in results.items():\n",
    "    print(f\"{model_name}:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09d4ca8-72bd-4309-a610-06555588596c",
   "metadata": {},
   "source": [
    "# SUMMARY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c682d71-9726-40a3-ab44-e30c585ffaf9",
   "metadata": {},
   "source": [
    "## 1. We have used three models: **Random Forest Classifier, Logistic Regression, SVC** on *breast cancer* dataset.\n",
    "## 2. **Logistic Regression**\n",
    "* **Accuracy:** 96.49%\n",
    "* **Precision:** 95.89%\n",
    "* **Recall:** 98.59%\n",
    "* **F1-score:** 97.22%\n",
    "* High precision and recall balance, making it a reliable and consistent model.\n",
    "## 3. **SVC (Support Vector Classifier)**\n",
    "* **Accuracy:** 94.74%\n",
    "* **Precision:** 92.21%\n",
    "* **Recall:** 100%\n",
    "* **F1-score:** 95.95%\n",
    "* Perfect recall indicates *zero false negatives*, which is excellent for use cases where missing a positive case is costly (e.g., medical diagnosis).\n",
    "* Slightly lower precision compared to others, suggesting more false positives.\n",
    "## 4. **Random Forest Classifier**\n",
    "* **Accuracy:** 96.49%\n",
    "* **Precision:** 95.89%\n",
    "* **Recall:** 98.59%\n",
    "* **F1-score:** 97.22%\n",
    "* Performs identically to Logistic Regression, showing high accuracy and a great balance of all metrics. Also benefits from model interpretability and feature importance.\n",
    "## 5. In terms of *accuracy* Logistic Regression and Random Forest Classifier have 96.49%.\n",
    "## 6. In terms of *recall* SVC is best as it has 100% recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae36403-7206-45d6-b45a-2cab413b6280",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
